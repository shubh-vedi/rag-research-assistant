"""
Contextual Retrieval
=====================
Anthropic's technique (2024) that reduces retrieval failures by 49%.

Problem: When you chunk a document, each chunk loses its context.
A chunk saying "The company earned $5M in Q3" is useless without knowing
which company and which year the document is about.

Solution: Before embedding, prepend each chunk with a short context
generated by an LLM that explains where the chunk sits in the document.

Before:
    "The company earned $5M in Q3."

After:
    "This chunk is from ACME Corp's 2024 Annual Report, discussing financial
    performance. The company earned $5M in Q3."

The contextually-enriched chunk embeds much better and retrieves more accurately.

Reference: https://www.anthropic.com/news/contextual-retrieval (2024)
"""

import logging

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from app.config import settings

logger = logging.getLogger(__name__)

CONTEXT_PROMPT = ChatPromptTemplate.from_template(
    """Here is the full document:
<document>
{document}
</document>

Here is a chunk from that document:
<chunk>
{chunk}
</chunk>

Write a short, concise context (2-3 sentences) that situates this chunk
within the overall document. Mention the document topic, relevant section,
and any key entities. Do NOT repeat the chunk content.

Context:"""
)


class ContextualRetrieval:
    """
    Enriches chunks with document-level context before embedding.

    How it works:
        1. For each chunk, the LLM reads the full document + the chunk
        2. It generates 2-3 sentences of context (what doc is this from,
           what section, what entities are discussed)
        3. Context is prepended to the chunk before embedding
        4. The enriched chunk embeds with much more semantic information

    Why it works:
        - "Earned $5M in Q3" is vague as a standalone embedding
        - "From ACME Corp 2024 report, Q3 financials. Earned $5M in Q3"
          clearly matches queries about "ACME Corp revenue"
    """

    def __init__(self, llm: ChatOpenAI | None = None):
        self.llm = llm or ChatOpenAI(
            model=settings.llm_model,
            temperature=0.0,
            openai_api_key=settings.openai_api_key,
        )
        self.chain = CONTEXT_PROMPT | self.llm

    def enrich_chunks(
        self,
        chunks: list[Document],
        full_document_text: str,
    ) -> list[Document]:
        """
        Add contextual information to each chunk.

        Args:
            chunks: Document chunks to enrich.
            full_document_text: The complete source document text.

        Returns:
            New list of Documents with context prepended to page_content.
        """
        if not chunks:
            return []

        # Truncate full document if too long (keep first ~6000 chars for context)
        doc_preview = full_document_text[:6000]
        if len(full_document_text) > 6000:
            doc_preview += "\n... [document truncated for context generation]"

        logger.info(f"[Contextual] Enriching {len(chunks)} chunks with document context")

        enriched: list[Document] = []
        for i, chunk in enumerate(chunks):
            try:
                result = self.chain.invoke({
                    "document": doc_preview,
                    "chunk": chunk.page_content,
                })
                context = result.content.strip()

                # Prepend context to chunk
                enriched_content = f"{context}\n\n{chunk.page_content}"

                enriched_doc = Document(
                    page_content=enriched_content,
                    metadata={
                        **chunk.metadata,
                        "has_context": True,
                        "context_preview": context[:100],
                        "original_length": len(chunk.page_content),
                        "enriched_length": len(enriched_content),
                    },
                )
                enriched.append(enriched_doc)

            except Exception as e:
                logger.warning(f"[Contextual] Failed on chunk {i}, keeping original: {e}")
                enriched.append(chunk)

        logger.info(f"[Contextual] Enriched {len(enriched)} chunks")
        return enriched
